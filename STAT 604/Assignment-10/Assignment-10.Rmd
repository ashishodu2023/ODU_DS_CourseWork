---
title: "Assignment-10"
author: "Ashish Verma"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Section 10.1 Problem #6
### Solution
Null Hypothesis: H0:mu1=mu2=m3=m4
Alternate Hypothesis: Ha: At-least one mu is different
```{R}
df_treatment<-4 -1
df_error<-40 -4

t1<-c(20.5,28.1,27.8,27.0,28.0,25.2,25.3,27.1,20.5,31.3)
t2<-c(26.3,24.0,26.2,20.2,23.7,34.0,17.1,26.8,23.7,24.9)
t3<-c(29.5,34.0,27.5,29.4,27.9,26.2,29.9,29.5,30.0,35.6)
t4<-c(36.5,44.2,34.1,30.3,31.4,33.1,34.1,32.9,36.3,25.5)
alpha<-0.01
data <- data.frame(
  value = c(t1, t2, t3, t4),
  treatment = factor(rep(c("t1", "t2", "t3", "t4"), each = 10))
)

result <- aov(value ~ treatment, data = data)

anova_summary <- summary(result)
f_value <- anova_summary[[1]][1,4]
p_value <- anova_summary[[1]][1,5]

cat("F-statistic:", f_value, "\n")
cat("P-value:", p_value, "\n")

cat('Pvalue',p_value)
if (p_value < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}
```
## Section 10.2 Problem #18
### Solution(a)

Null Hypothesis H0:mu1=mu2=mu3=mu4=mu5
Alternate Hypothesis Ha:Not all means are equal.

```{R}
t1<-c(13,17,7,14)
t2<-c(21,13,20,17)
t3<-c(18,15,20,17)
t4<-c(7,11,18,10)
t5<-c(6,11,15,8)
alpha<-0.05
data <- data.frame(
  value = c(t1, t2, t3, t4, t5),
  treatment = factor(rep(c("t1", "t2", "t3", "t4", "t5"), each=4))
)
result <- aov(value ~ treatment, data = data)
anova_summary <- summary(result)
f_value <- anova_summary[[1]][1,4]
p_value <- anova_summary[[1]][1,5]

cat("F-statistic:", f_value, "\n")
cat("P-value:", p_value, "\n")
if (p_value < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}

p1<-c(13,21,18,7,6)
p2<-c(17,13,15,11,11)
p3<-c(7,20,20,18,15)
p4<-c(14,17,17,10,8)

# Combine the data into a data frame
data <- data.frame(
  values = c(p1, p2, p3, p4),
  group = factor(rep(c("p1", "p2", "p3", "p4"), each=5))
)
result_p <- aov(values ~ group, data = data)

anova_summary <- summary(result_p)
f_value_p <- anova_summary[[1]][1,4]
p_value_p <- anova_summary[[1]][1,5]

cat("F-statistic:", f_value_p, "\n")
cat("P-value:", p_value_p, "\n")
if (p_value_p < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}
```
### Solution(b)
```{R}
tukey_result <- TukeyHSD(result)
tukey_result_p <- TukeyHSD(result_p)
tukey_result
tukey_result_p
p_values <- tukey_result$p.adj
p_values_p <- tukey_result_p$p.adj
```
Thus, both the procedures do not give the same result. None of the can be said to differ significantly from one another.

## Section 10.3 Problem #26
### Solution(a)
```{R}
ip<-c(14.1,13.6,14.4,14.3)
pk<-c(12.8,12.5,13.4,13.0,12.3)
bb<-c(13.5,13.4,14.1,14.3)
cf<-c(13.2,12.7,12.6,13.9)
mz<-c(16.8,17.2,16.4,17.3,18.0)
fl<-c(18.1,17.2,18.7,18.4)

library(dplyr)

# Creating a dataframe with the given data
data <- data.frame(
  Value = c(14.1, 13.6, 14.4, 14.3, 12.8, 12.5, 13.4, 13.0, 12.3, 
            13.5, 13.4, 14.1, 14.3, 13.2, 12.7, 12.6, 13.9, 16.8, 
            17.2, 16.4, 17.3, 18.0, 18.1, 17.2, 18.7, 18.4),
  Group = rep(c("ip", "pk", "bb", "cf", "mz", "fl"), 
              times = c(4, 5, 4, 4, 5, 4))
)

anova_result <- aov(Value ~ Group, data = data)
anova_summary <- summary(anova_result)

F_value <- anova_summary[[1]]$F[1]
cat("F statistics results",F_value)
p_value <- anova_summary[[1]]$Pr[1]
cat("Pvalue",p_value)
if (p_value < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}
```
### Solution(b)
```{R}
turkey_result <- TukeyHSD(anova_result)
print(turkey_result)
```
### Solution(c)
```{R}
thetacap<-(mean(ip)+mean(pk)+mean(bb)+mean(cf))/4 - (mean(mz)+mean(fl))/2
alpha<-0.01
SSC<-((1/16)/4)+((1/16)/5)+((1/16)/4)+((1/16)/4)+((1/4)/5)+((1/4)/4)
tcrit<-qt(1 - alpha/2,20)
margin<-sqrt(tcrit*SSC)
lower_limit<-thetacap - margin
upper_limit<-thetacap + margin

cat("The confidence interval is",lower_limit,upper_limit)
```

## Section 12.2 Problem #16
### Solution(a)
```{R}
x<-c(5,12,14,17,23,30,40,47,55,67,72,81,96,112,127)
y<-c(4,10,13,15,15,25,27,46,38,46,53,70,82,99,100)
plot(x,y,main="Scatter Plot",pch=19,xlab="Rainfall Volume",ylab="Ranoff Volume")
```
The scatter plot show linear relationship.

### Solution(b)
```{R}
model = lm(y~x)
Bo <- coefficients(model)[1]
B1 <- coefficients(model)[2]
cat("The Bo",Bo)
cat("The B1",B1)
```
### Solution(c)
```{R}
ypred<-Bo+B1*50
print(ypred)
```
### Solution(d)
```{R}
A=summary(model)
point_estimate <- A$sigma
cat("The point esitmate of simga",point_estimate)
```
### Solution(e)
```{R}
R2<-A$r.squared
cat(round(R2,4))
```

## Section 12.3 Problem #34
### Soltuion(a)
The equation of the least squares line is: y = 4.8567 - 0.0747x. The slope of the line, -0.0747, indicates that as air void increases by 1%, the dielectric constant decreases by approximately 0.0747.

### Solution(b)
The multiple R-squared value of 0.7797 indicates that approximately 77.97% of the observed variation in the dielectric constant can be explained by the linear relationship between the dielectric constant and air void.

### Soltuion(c)
Null Hypothesis H0:beta=0

Alternate Hypothesis Ha:beta!=0
```{R}
alpha<-0.01
data <- data.frame(y = c(4.55, 4.49, 4.50, 4.47, 4.47, 4.45, 4.40, 4.34, 4.43, 4.43, 4.42, 4.40, 4.33, 4.44, 4.40, 4.26, 4.32, 4.34),x = c(4.35, 4.79, 5.57, 5.20, 5.07, 5.79, 5.36, 6.40, 5.66, 5.90, 6.49, 5.70, 6.49, 6.37, 6.51, 7.88, 6.74, 7.08))
model <- lm(y ~ x, data = data)
summary(model)
coef_summary <- summary(model)$coefficients
coef_x <- coef_summary["x", ]
t_value <- coef_x["t value"]
p_value <- coef_x["Pr(>|t|)"]

cat("T testing results",t_value)
cat("Pvalue results",p_value)

if (p_value < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}
```
### Soltuion(d)
Null Hypothesis:H0:beta=-0.064
Alternate Hypothesis:Ha:beta<-0.064
```{R}
beta<--0.064
betanotcap<-0.074676   #from table
SSE <- coef_x["Std. Error"]
ttesting <- (-betanotcap+beta)/SSE
p_value <- pt(ttesting,16)
cat("P_value results",p_value)
if (p_value < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}
```

## Section 12.4 Problem #46
### Soluion(a)
```{R}
x <- c(0.718, 0.808, 0.924, 1.000, 0.667, 0.529, 0.514, 0.559,
       0.766, 0.470, 0.726, 0.762, 0.666, 0.562, 0.378, 0.779,
       0.674, 0.858, 0.406, 0.927, 0.311, 0.319, 0.518, 0.687,
       0.907, 0.638, 0.234, 0.781, 0.326, 0.433, 0.319, 0.238)
y <- c(0.428, 0.480, 0.493, 0.978, 0.318, 0.298, 2.224, 0.198,
       0.326, 2.336, 0.765, 0.190, 0.066, 2.221, 2.898, 0.836,
       0.126, 0.305, 2.577, 0.779, 2.707, 2.610, 2.648, 2.145,
       1.007, 2.090, 21.132, 0.538, 21.098, 2.581, 2.862, 2.551)
SXX<-1.48193150
SYY<-11.82637622
SXY<-3.83071088
sigma_xi<-19.404
sigma_yi<--.594
sigma_y2<-11.835795
sigma_xy<-3.497811

betacap<-SXY/SXX
betanotcap<-(sigma_yi/32) - (betacap*sigma_xi/32)
cat("The fitted simple regression is",betacap,betanotcap)

SST<-sigma_y2 - (sigma_yi^2)/32
SSE<-sigma_y2 -(betanotcap*sigma_yi) - (betacap*sigma_xy)
R2<- 1 - (SSE/SST)
cat("The proportion of observed variation is given by",R2)
```
### Soluion(b)
```{R}
alpha<-0.05
SSE<-sigma_y2 -(betanotcap*sigma_yi) - (betacap*sigma_xy)
SE<- sqrt(SSE/(32-2))
sb1 = SE/sqrt(SXX)
cat("SB1",sb1)
df<-30
tcrit<- qt(1 - alpha/2,df)
cat("Tcrit",tcrit)
lower_limit<-betacap - (tcrit*sb1)
upper_limit<-betacap + (tcrit*sb1)
cat("The 95% confidence interval are",lower_limit,upper_limit)

```
### Soluion(c)
```{R}
predicted_x<-0.6
yprime<-betanotcap+(betacap*predicted_x)
cat("Yprime",yprime)
degree_of_freedom<-30
alpha<-0.05
tcrit<- qt(1 - alpha/2,df)
sqrt_result<-sqrt(0.03125+((predicted_x - (sigma_xi/32))^2)/SXX)
margin<-tcrit*SE*sqrt_result
lower_limit<- yprime - margin
upper_limit<- yprime + margin
cat("The 95% confidence interval are",lower_limit,upper_limit)
```
### Soluion(d)
```{R}
predicted_x<-0.6
yprime<-betanotcap+(betacap*predicted_x)
cat("Yprime",yprime)
degree_of_freedom<-30
alpha<-0.05
tcrit<- qt(1 - alpha/2,df)
sqrt_result<-sqrt(1.03125+((predicted_x - (sigma_xi/32))^2)/SXX)
margin<-tcrit*SE*sqrt_result
lower_limit<- yprime - margin
upper_limit<- yprime + margin
cat("The 95% confidence interval are",lower_limit,upper_limit)
```
### Soluion(e)
```{R}
predicted_x<-0.7
yprime<-betanotcap+(betacap*predicted_x)
cat("Yprime",yprime)
degree_of_freedom<-30
alpha<-0.05
tcrit<- qt(1 - alpha/2,df)
sqrt_result<-sqrt(0.03125+((predicted_x - (sigma_xi/32))^2)/SXX)
margin<-tcrit*SE*sqrt_result
lower_limit<- yprime - margin
upper_limit<- yprime + margin
cat("The 95% confidence interval are",lower_limit,upper_limit)
```
Since 0 is not contained in interval, there is enough evidence to conclude that true average astringency for a tannin concentration of 0.75 is something other than 0.


## Section 12.5 Problem #58

### Solution(a)
```{R}
x<-c(4200,3600,3750,3675,4050,2770,4870,4500,3450,2700,3750,3300)
y<-c(370,340,375,310,350,200,400,375,285,225,345,285)
n<-length(x)
xbar<-mean(x)
ybar<-mean(y)

Sxx <- sum(x^2)-(sum(x)^2)/n
Syy <- sum(y^2)-(sum(y)^2)/n
Sxy <- sum(x*y) - (sum(x)*sum(y))/n

corr_r<-Sxy/sqrt(Sxx*Syy)
cat('Correlation coefficient',corr_r)
```
### Solution(b)
Same output.The correlation coefficient is independent of origin and scale. It won't change by adding, subtraction, multiplying and dividing.

### Solution(c)
Same output.The correlation coefficient is independent of origin and scale. It won't change by adding, subtraction, multiplying and dividing

### Solution(d)
Null Hypothesis:H0:p=0
Alternate Hypothesis Ha:p!=0
```{R}
qqnorm(x)
qqline(x)
alpha<-0.05
qqnorm(y)
qqline(y)
n<-12
df<- n -2
ttesting<- corr_r * sqrt(df)/sqrt(1-(corr_r^2))
cat("T Statics results",ttesting)
p_value <- 2 * (1 - pt(abs(ttesting), df))
cat("P value",p_value)
if (p_value < alpha) {
  print("Reject the null hypothesis")
} else {
  print("Failed to reject the null hypothesis")
}
```