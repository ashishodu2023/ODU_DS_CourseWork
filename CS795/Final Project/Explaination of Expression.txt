Expression 1:
∇θc​​J(θc​) is the gradient of the objective function J(θc)J(θc​) with respect to the policy parameters θcθc​. This gradient represents the direction in which the parameters should be updated to increase the expected cumulative reward.

∑t=1T∑t=1T​ is a summation over the time steps TT, indicating that the gradient is computed by summing over all time steps.

EP(a1:T;θc)EP(a1:T​;θc​)​ denotes the expectation with respect to the distribution P(a1:T;θc)P(a1:T​;θc​), representing the expected value over all possible sequences of actions generated by the policy parameterized by θcθc​.

∇θclog⁡P(at∣a(t−1):1;θc)∇θc​​logP(at​∣a(t−1):1​;θc​) is the gradient of the log probability of taking action atat​ given the history of actions a(t−1):1a(t−1):1​ and the policy parameterized by θcθc​.

RR is the cumulative reward obtained after taking actions a1:Ta1:T​.



Expresion 2:
m1​: This term represents the average over mm independent samples. It's common to use mini-batches in training, and this term scales the sum to obtain the average.

∑k=1m∑k=1m​: This is the summation over the mm independent samples or trajectories.

∑t=1T∑t=1T​: This represents the summation over time steps within each trajectory.

5θclog⁡P(at∣a(t−1):1;θc)5θc​logP(at​∣a(t−1):1​;θc​): This part is similar to the log probability term in the standard policy gradient. It measures the log probability of taking action atat​ given the history of actions a(t−1):1a(t−1):1​ and is scaled by 5θc5θc​.

(Rk−b)(Rk​−b): This term adjusts the reward obtained in trajectory kk by subtracting a baseline bb. The baseline is subtracted to reduce the variance in the gradient estimate. The baseline can be a value that is independent of the chosen actions, such as the average reward over all trajectories.